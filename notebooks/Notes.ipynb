{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $r(s,a,s')$ reward\n",
    "- State-action value function: $Q^{\\pi}(s,a) = E_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t,a_t,s_{t+1})|s_0=s,a_0=a]$\n",
    "- Value function $V^{\\pi}(s) = E_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t,a_t,s_{t+1})|s_0=s]$\n",
    "\n",
    "Evaluation operator:\n",
    "- $T^\\pi V\\left(s\\right) = r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V(s') \\right]$\n",
    "- $T^\\pi Q\\left(s,a\\right) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ Q(s',\\pi(s')) \\right]$\n",
    "\n",
    "Bellman evaluation operator:\n",
    "- $Q = T^{\\pi}Q$\n",
    "\n",
    "Bellam optimality equation:\n",
    "- $Q = T^*Q$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
